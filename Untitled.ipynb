{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c5b797c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.signal import butter, sosfilt, sosfreqz, resample, sosfiltfilt\n",
    "import scipy.io\n",
    "import random\n",
    "import json\n",
    "from livelossplot import PlotLosses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4a3f8897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "        nyq = 0.5 * fs\n",
    "        low = lowcut / nyq\n",
    "        high = highcut / nyq\n",
    "        sos = butter(order, [low, high], analog=False, btype='band', output='sos')\n",
    "        return sos\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=2):\n",
    "        sos = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "        y = sosfiltfilt(sos, data)\n",
    "        return y\n",
    "\n",
    "class MODA_proc(Dataset):\n",
    "    def __init__(self, input_path = '/scratch/s174411/center_width/1D_MASS_MODA_processed/input/', label_path = '/scratch/s174411/center_width/1D_MASS_MODA_processed/labels/'):\n",
    "        self.input_path = input_path\n",
    "        self.label_path = label_path\n",
    "        self.input_dict = {}\n",
    "        self.label_dict = {}\n",
    "        temp_input_list = []\n",
    "        temp_output_list = []\n",
    "        for root, dirs, files in os.walk(self.input_path):\n",
    "            for name in files:\n",
    "                if name.endswith('npy'):\n",
    "                    temp_input_list.append(os.path.join(root, name))\n",
    "                    self.input_dict[int(name[:-4])] = os.path.join(root, name)\n",
    "\n",
    "        for root, dirs, files in os.walk(self.label_path):\n",
    "            for name in files:\n",
    "                if name.endswith('json'):\n",
    "                    temp_output_list.append(os.path.join(root, name))\n",
    "                    self.label_dict[int(name[:-5])] = os.path.join(root, name)\n",
    "\n",
    "        self.master_path_list = []\n",
    "        \n",
    "        for in_path in temp_input_list:\n",
    "            for la_path in temp_output_list:\n",
    "                if in_path[-16:-3] == la_path[-17:-4]:\n",
    "                    self.master_path_list.append((in_path,la_path))\n",
    "                \n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.master_path_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #print(self.input_dict[idx])\n",
    "        model_input, labels = self.master_path_list[idx]\n",
    "        eeg_input = np.load(model_input)\n",
    "        eeg_input = resample(eeg_input, 100*115)\n",
    "        eeg_input = butter_bandpass_filter(eeg_input, 0.3, 30, 100, 10)\n",
    "        # Standardize\n",
    "        eeg_input = (eeg_input - np.mean(eeg_input))/np.std(eeg_input)\n",
    "\n",
    "        eeg_input = torch.FloatTensor(eeg_input)\n",
    "        eeg_input = eeg_input[None, :]\n",
    "\n",
    "        #print('dataloader shape')\n",
    "\n",
    "        #image = np.array(cv2.imread(self.input_dict[idx]))\n",
    "        #image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "        #image = read_image(self.input_dict[idx])\n",
    "        f = open(labels)\n",
    "        \n",
    "        labels = (json.load(f))\n",
    "        f.close()\n",
    "\n",
    "\n",
    "        input_length = int(100*115)\n",
    "        sumo_label_format = np.zeros(input_length)\n",
    "        for bbox in labels['boxes']:\n",
    "            \n",
    "            box_start = bbox[0] - bbox[1]/2\n",
    "            box_end = bbox[0] + bbox[1]/2\n",
    "            box_start_scaled = int(box_start * input_length)\n",
    "            box_end_scaled = int(box_end * input_length)\n",
    "            sumo_label_format[box_start_scaled:box_end_scaled] = 1\n",
    "\n",
    "        spindle = False\n",
    "        for sample in sumo_label_format:\n",
    "            if sample == 1:\n",
    "                spindle = True\n",
    "\n",
    "        if not spindle:\n",
    "            print(\"found\")\n",
    "        #print(type(sumo_label_format))\n",
    "        sumo_label_format = torch.FloatTensor(sumo_label_format)\n",
    "        #print(sumo_label_format.shape)\n",
    "\n",
    "        return eeg_input, sumo_label_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "09bdb3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class u_net_backbone(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        n_groups = 8\n",
    "\n",
    "        # DOWNSAMPLING\n",
    "\n",
    "        #self.pad = nn.Conv1d(1, 1, kernel_size = 1, stride = 2)\n",
    "        \n",
    "        \n",
    "        # ENCODER GROUND LEVEL (LEVEL 1)\n",
    "        self.conv_1_1 = nn.Conv1d(1, 16, kernel_size = 5, dilation = 2, padding = 'same')\n",
    "        self.batch_1_1 = nn.GroupNorm(n_groups, 16)\n",
    "\n",
    "        self.conv_1_2 = nn.Conv1d(16, 16, kernel_size = 5, dilation = 2, padding = 'same')\n",
    "        self.batch_1_2 = nn.GroupNorm(n_groups, 16)\n",
    "\n",
    "        # LEVEL 2\n",
    "        self.pool_1 = nn.MaxPool1d(kernel_size = 4)\n",
    "\n",
    "        self.conv_2_1 = nn.Conv1d(16, 32, kernel_size = 5, dilation = 2, padding = 'same')\n",
    "        self.batch_2_1 = nn.GroupNorm(n_groups, 32)\n",
    "\n",
    "        self.conv_2_2 = nn.Conv1d(32, 32, kernel_size = 5, dilation = 2, padding = 'same')\n",
    "        self.batch_2_2 = nn.GroupNorm(n_groups, 32)\n",
    "\n",
    "\n",
    "        # ENCODER BOTTOM LEVEL\n",
    "        self.pool_2 = nn.MaxPool1d(kernel_size = 4)\n",
    "\n",
    "        self.conv_3_1 = nn.Conv1d(32, 64, kernel_size = 5, dilation = 2, padding = 'same')\n",
    "        self.batch_3_1 = nn.GroupNorm(n_groups, 64)\n",
    "\n",
    "        self.conv_3_2 = nn.Conv1d(64, 64, kernel_size = 5, dilation = 2, padding = 'same')\n",
    "        self.batch_3_2 = nn.GroupNorm(n_groups, 64)\n",
    "\n",
    "        # DECODER LEVEL 2\n",
    "        # UPSAMPLING\n",
    "        self.upsample_2 = nn.Upsample(scale_factor = 4, mode = 'nearest')\n",
    "        self.conv_2_3 = nn.Conv1d(64, 32, kernel_size = 4, dilation = 1, padding = 'same')\n",
    "\n",
    "        # \n",
    "        self.conv_2_4 = nn.Conv1d(64, 32, kernel_size = 5, dilation = 1, padding = 'same')\n",
    "        self.batch_2_4 = nn.GroupNorm(n_groups, 32)\n",
    "\n",
    "        self.conv_2_5 = nn.Conv1d(32, 32, kernel_size = 5, dilation = 1, padding = 'same')\n",
    "        self.batch_2_5 = nn.GroupNorm(n_groups, 32)\n",
    "\n",
    "        \n",
    "        # DECODER GROUND LEVEL (LEVEL 1)\n",
    "        # UPSAMPLING\n",
    "        self.upsample_1 = nn.Upsample(scale_factor = 4, mode = 'nearest')\n",
    "        self.conv_1_3 = nn.Conv1d(32, 16, kernel_size = 4, dilation = 1, padding = 'same')\n",
    "\n",
    "        # \n",
    "        self.conv_1_4 = nn.Conv1d(32, 16, kernel_size = 5, dilation = 1, padding = 'same')\n",
    "        self.batch_1_4 = nn.GroupNorm(n_groups, 16)\n",
    "\n",
    "        self.conv_1_5 = nn.Conv1d(16, 16, kernel_size = 5, dilation = 1, padding = 'same')\n",
    "        self.batch_1_5 = nn.GroupNorm(n_groups, 16)\n",
    "\n",
    "\n",
    "        self.conv_1_6 = nn.Conv1d(16, 2, kernel_size = 1, dilation = 1)\n",
    "        \n",
    "    def forward(self, tensor_list):\n",
    "        #print(tensor_list.shape)\n",
    "        # DOWNSAMPLING\n",
    "        #downsampled_input = self.downsample(tensor_list)\n",
    "        #extrapolation = int(np.ceil(tensor_list.shape[1] / (4*4*4)) * (4*4*4) - tensor_list.shape[1])\n",
    "        padded_input = F.pad(tensor_list, (2, 2), mode='reflect')\n",
    "        #padded_input = tensor_list\n",
    "\n",
    "        # GROUND LEVEL FORWARD\n",
    "        level_1 = self.batch_1_1(F.relu(self.conv_1_1(padded_input)))\n",
    "        level_1 = self.batch_1_2(F.relu(self.conv_1_2(level_1)))\n",
    "\n",
    "        # POOLING AND LEVEL 2\n",
    "        level_1_down = self.pool_1(level_1)\n",
    "        level_2 = self.batch_2_1(F.relu(self.conv_2_1(level_1_down)))\n",
    "        level_2 = self.batch_2_2(F.relu(self.conv_2_2(level_2)))\n",
    "\n",
    "        # POOLING AND BOTTOM LEVEL\n",
    "        level_2_down = self.pool_2(level_2)\n",
    "        level_3 = self.batch_3_1(F.relu(self.conv_3_1(level_2_down)))\n",
    "        level_3 = self.batch_3_2(F.relu(self.conv_3_2(level_3)))\n",
    "\n",
    "        # UPSAMPLING AND FEATURE FUSION (LEVEL 2)\n",
    "        level_3_upsampled = self.upsample_2(level_3)\n",
    "        level_2_up = self.conv_2_3(level_3_upsampled)\n",
    "        \n",
    "        #print(level_2.shape)\n",
    "        #print(level_3_upsampled.shape)\n",
    "        dec_level_2 = torch.cat((level_2, level_2_up), 1)\n",
    "\n",
    "        dec_level_2 = self.batch_2_4(F.relu(self.conv_2_4(dec_level_2)))\n",
    "        dec_level_2 = self.batch_2_5(F.relu(self.conv_2_5(dec_level_2)))\n",
    "\n",
    "        # UPSAMPLING AND FEATURE FUSION (UPPER LEVEL)\n",
    "        level_2_upsampled = self.upsample_1(dec_level_2)\n",
    "        level_1_up = self.conv_1_3(level_2_upsampled)\n",
    "        \n",
    "        dec_level_1 = torch.cat((level_1, level_1_up), 1)\n",
    "\n",
    "        dec_level_1 = self.batch_1_4(F.relu(self.conv_1_4(dec_level_1)))\n",
    "        dec_level_1 = self.batch_1_5(F.relu(self.conv_1_5(dec_level_1)))\n",
    "\n",
    "        dec_level_1 = self.conv_1_6(dec_level_1)\n",
    "\n",
    "\n",
    "        diff = dec_level_1.shape[2] - tensor_list.shape[2]\n",
    "        crop_dims = [diff // 2, diff // 2 + diff % 2]\n",
    "\n",
    "        if crop_dims[1] == 0:\n",
    "            dec_level_1 = dec_level_1[:, :, crop_dims[0]:]\n",
    "        else:\n",
    "            dec_level_1 = dec_level_1[:, :, crop_dims[0]:-crop_dims[1]]\n",
    "\n",
    "        # Not used when calculating loss\n",
    "        #smooth = F.avg_pool1d(dec_level_1, 42, stride=1)\n",
    "\n",
    "        return dec_level_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "06afbab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneralizedDiceLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Compute the generalised Dice loss defined in:\n",
    "        Sudre, C. et al. (2017) Generalised Dice overlap as a deep learning loss function for highly unbalanced\n",
    "        segmentations. DLMIA 2017. https://arxiv.org/pdf/1707.03237.pdf\n",
    "    Adapted from:\n",
    "        https://github.com/Project-MONAI/MONAI/blob/0.5.2/monai/losses/dice.py#L216\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    reduction : {'mean', 'sum', 'none'}, optional\n",
    "        Specifies the reduction to apply to the output. The sum of the output will be divided by the number of\n",
    "        elements in the output ('mean'), the output will be summed ('sum') or no reduction will be applied ('none').\n",
    "        Default is 'mean'.\n",
    "    smooth : float, optional\n",
    "        A small constant added to the numerator and denominator to avoid zero and nan.\n",
    "    use_weight: bool, optional\n",
    "        When true, use class weights as originally proposed by Sudre et al.\n",
    "    softmax : bool, optional\n",
    "        When True, apply a softmax function to the prediction.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, reduction: str = 'mean', smooth: float = 1e-5, use_weight: bool = True, softmax: bool = True):\n",
    "        super(GeneralizedDiceLoss, self).__init__()\n",
    "\n",
    "        self.reduction = reduction\n",
    "        self.smooth = smooth\n",
    "        self.use_weight = use_weight\n",
    "        self.softmax = softmax\n",
    "\n",
    "    @staticmethod\n",
    "    def get_onehot_encoding(y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "        shp_x = y_pred.shape\n",
    "        shp_y = y_true.shape\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_true = y_true.long()\n",
    "\n",
    "            if shp_x == shp_y:\n",
    "                return y_true  # y_true is already in one hot encoding\n",
    "            else:\n",
    "                y_onehot = F.one_hot(y_true, num_classes=shp_x[1])  # one hot encoding in format [N,K,C]\n",
    "                y_onehot = y_onehot.permute(0, 2, 1)  # transform to format [N,C,K]\n",
    "\n",
    "                return y_onehot\n",
    "\n",
    "    def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply the forward pass of the generalized dice loss function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_pred : torch.Tensor\n",
    "            The predicted logits (or probabilities if `self.softmax` is False) in shape [N,C,K] where N is the batch\n",
    "            size, C the number of classes and K the number of elements/steps in each observation.\n",
    "        y_true : torch.Tensor\n",
    "            The target labels in shape [N,K] or as one hot encoded vector in format [N,C,K].\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss : torch.Tensor\n",
    "            The calculated loss as scalar or in shape [N] if `self.reduction` is 'none'.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If `self.reduction` is not one of {'mean', 'sum', 'none'}.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.softmax:\n",
    "            y_pred = F.softmax(y_pred, dim=1)\n",
    "\n",
    "        y_onehot = self.get_onehot_encoding(y_pred, y_true)\n",
    "\n",
    "        # calculate intersection and union and sum them over the K steps in each observation; shape [N,C]\n",
    "        intersection = (y_pred * y_onehot).sum(dim=2)\n",
    "        union = (y_pred + y_onehot).sum(dim=2)\n",
    "\n",
    "        if self.use_weight:\n",
    "            # class weights using the inverse of each label volume; shape [N,C]\n",
    "            w = 1 / y_onehot.sum(dim=2)**2\n",
    "            # if one class doesn't contain any labels, its weight is set to 1.0 (as if it would contain exactly one\n",
    "            # label)\n",
    "            w[torch.isinf(w)] = 1.0\n",
    "\n",
    "            # apply the weights on intersection and union and sum over the classes; shape [N]\n",
    "            intersection = (w * intersection).sum(dim=1)\n",
    "            union = (w * union).sum(dim=1)\n",
    "        else:\n",
    "            # sum intersection and union over the classes; shape [N]\n",
    "            intersection = intersection.sum(dim=1)\n",
    "            union = union.sum(dim=1)\n",
    "\n",
    "        # calculate dice coefficient and generalized dice loss using a small number to prevent zero/nan; shape [N]\n",
    "        dice = (2.0 * intersection + self.smooth) / (union + self.smooth)\n",
    "        gdl = 1.0 - dice\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            gdl = gdl.mean()  # average over the batch and channel; scalar\n",
    "        elif self.reduction == 'sum':\n",
    "            gdl = gdl.sum()  # sum over the batch and channel; scalar\n",
    "        elif self.reduction == 'none':\n",
    "            pass  # unmodified losses per batch; shape [N]\n",
    "        else:\n",
    "            raise ValueError(f'Unsupported reduction: {self.reduction}, available options are [\"mean\", \"sum\", \"none\"].')\n",
    "\n",
    "        return gdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "01c41f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_spindle_list(list_of_spindles, target = False):\n",
    "    refined_spindle_list = []\n",
    "    for spindle in list_of_spindles:\n",
    "        start = spindle[0]/(100)\n",
    "        end = spindle[1]/(100)\n",
    "        if ((end-start) < 0.3 and not target):\n",
    "            continue\n",
    "        else:\n",
    "            refined_spindle_list.append((start, end))\n",
    "\n",
    "    return refined_spindle_list\n",
    "\n",
    "\n",
    "\n",
    "def iou(out,tar):\n",
    "    out_box_start = out[0]\n",
    "    out_box_end = out[1]\n",
    "\n",
    "    tar_box_start = tar[0]\n",
    "    tar_box_end = tar[1]\n",
    "\n",
    "    overlap_start = max(out_box_start, tar_box_start)\n",
    "    overlap_end = min(out_box_end, tar_box_end)\n",
    "    union_start = min(out_box_start, tar_box_start)\n",
    "    union_end = max(out_box_end, tar_box_end)\n",
    "\n",
    "    return ((overlap_end - overlap_start)/(union_end-union_start))\n",
    "\n",
    "def overlap(out, tar, threshold):\n",
    "    out_box_start = out[0]\n",
    "    out_box_end = out[1]\n",
    "\n",
    "    tar_box_start = tar[0]\n",
    "    tar_box_end = tar[1]\n",
    "\n",
    "    overlap_start = max(out_box_start, tar_box_start)\n",
    "    overlap_end = min(out_box_end, tar_box_end)\n",
    "    union_start = min(out_box_start, tar_box_start)\n",
    "    union_end = max(out_box_end, tar_box_end)\n",
    "\n",
    "    if (overlap_end - overlap_start) >= (threshold * (tar_box_end-tar_box_start)):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d0e1475a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(outputs, targets):\n",
    "    \n",
    "    # Loop through batches to compute F1 score through training.\n",
    "\n",
    "    \n",
    "    F1_list = []\n",
    "    temp_tp = 0\n",
    "    total_spindle_count = 0\n",
    "    total_pred_count = 0\n",
    "\n",
    "    \n",
    "    \n",
    "    for i in range(outputs.shape[0]):\n",
    "\n",
    "        pred = out_to_vector(outputs[i,:,:].cpu())\n",
    "\n",
    "        pred_spindles = vector_to_spindle_list(pred)\n",
    "        #print(len(pred_spindles))\n",
    "\n",
    "        #pred_spindles = refine_spindle_list(pred_spindles)\n",
    "        #print(len(pred_spindles))\n",
    "\n",
    "        \n",
    "        TP = 0\n",
    "\n",
    "        target = targets[i]\n",
    "        t_spindles = vector_to_spindle_list(target.cpu())\n",
    "        #t_spindles = refine_spindle_list(t_spindles)\n",
    "\n",
    "        total_spindle_count += len(t_spindles)\n",
    "        batch_spindle_count = len(t_spindles)\n",
    "\n",
    "        if len(t_spindles) == 0:\n",
    "            spindle = False\n",
    "            for l, sample in enumerate(target):\n",
    "                if sample == 1:\n",
    "                    spindle = True\n",
    "                    print(l)\n",
    "            if spindle:\n",
    "                print('not found')\n",
    "                print(vector_to_spindle_list(target.cpu(), debug = True))\n",
    "                print(len(target))\n",
    "        batch_pred_count = len(pred_spindles)\n",
    "        for k in range(len(t_spindles)):\n",
    "            tar_box = t_spindles[k]\n",
    "            #print(tar_box)\n",
    "            \n",
    "            best_match = -1\n",
    "\n",
    "            if len(pred_spindles) == 0:\n",
    "                continue\n",
    "            \n",
    "            for j,out_box in enumerate(pred_spindles):\n",
    "\n",
    "                if iou(out_box, tar_box) > iou(pred_spindles[best_match], tar_box):\n",
    "                    best_match = j\n",
    "            #print(pred_spindles[best_match])\n",
    "            #print(tar_box)\n",
    "            if iou(pred_spindles[best_match],tar_box) > 0.2:\n",
    "                TP +=1\n",
    "            \n",
    "\n",
    "        FP = batch_pred_count - TP\n",
    "        FN = batch_spindle_count - TP\n",
    "        \n",
    "        if (TP + FP) == 0:\n",
    "            PRECISION = TP\n",
    "        else:\n",
    "            PRECISION = (TP)/(TP + FP)\n",
    "        \n",
    "        RECALL = (TP)/(TP+FN)\n",
    "\n",
    "        if (PRECISION + RECALL) == 0:\n",
    "            F1_list.append(0)\n",
    "        else:\n",
    "            F1_list.append((2 * PRECISION * RECALL)/(PRECISION + RECALL))\n",
    "        \n",
    "        temp_tp += TP\n",
    "\n",
    "\n",
    "    F1_list = np.asarray(F1_list)\n",
    "    #print(\"F1 MEAN:\", np.mean(F1_list), \" F1 STD:\", np.std(F1_list), \" TP:\", temp_tp, \" FP:\", FP, \" Number of spindles:\", total_spindle_count)\n",
    "    return (np.mean(F1_list), np.std(F1_list), temp_tp, FP, total_spindle_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ad809311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_to_vector(output):\n",
    "    moving_avg = 42\n",
    "    s = moving_avg - 1\n",
    "    vector = F.pad(output, (s // 2, s // 2 + s % 2), mode='constant', value=0)\n",
    "\n",
    "    vector_smoothed =  F.avg_pool1d(vector, moving_avg, stride=1)\n",
    "    vector_softmax = F.softmax(vector_smoothed, dim=1)\n",
    "    top_p, top_class = vector_softmax.topk(1, dim = 0)\n",
    "\n",
    "    return top_class[0]\n",
    "\n",
    "\n",
    "def vector_to_spindle_list(vector, debug = False):\n",
    "    \n",
    "    prev_class = 0\n",
    "    list_of_spindles = []\n",
    "    vector = vector.numpy()\n",
    "    for i, instance_class in enumerate(vector):\n",
    "        if (instance_class == 1 and prev_class == 1):\n",
    "            prev_class = 1\n",
    "            if (i+1 == len(vector)):\n",
    "                spindle.append(i)\n",
    "                list_of_spindles.append(spindle)\n",
    "            continue\n",
    "\n",
    "        if (instance_class == 1 and prev_class == 0):\n",
    "            spindle = []\n",
    "            spindle.append(i)\n",
    "            prev_class = 1\n",
    "            continue\n",
    "\n",
    "        if (instance_class == 0 and prev_class == 1):\n",
    "            spindle.append(i)\n",
    "            list_of_spindles.append(spindle)\n",
    "            prev_class = 0\n",
    "            continue\n",
    "\n",
    "        prev_class = 0\n",
    "    return list_of_spindles\n",
    "        \n",
    "    #print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be304efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    \"train\": trainloader,\n",
    "    \"validation\": testloader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fd527b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = MODA_proc(input_path = '/scratch/s174411/full_segments/TRAIN/input/', label_path = '/scratch/s174411/full_segments/TRAIN/labels/')\n",
    "dataset_val = MODA_proc(input_path = '/scratch/s174411/full_segments/VAL/input/', label_path = '/scratch/s174411/full_segments/VAL/labels/')\n",
    "\n",
    "data_loader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "data_loader_val = DataLoader(dataset_val, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eb7e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, num_epochs=10):\n",
    "    liveloss = PlotLosses()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    prev_f1 = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        logs = {}\n",
    "        f1_mean_run = []\n",
    "        f1_std_run = []\n",
    "        TP_run = []\n",
    "        FP_run = []\n",
    "        total_spindle_run = []\n",
    "        for phase in ['train', 'validation']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                running_loss += loss.detach() * inputs.size(0)\n",
    "                if (epoch % 100 == 0):\n",
    "                    f1_mean, f1_std, TP, FP, total_spindle_count = f1_score(outputs, labels)\n",
    "                    f1_mean_run.append(f1_mean)\n",
    "                    f1_std_run.append(f1_std)\n",
    "                    TP_run.append(TP)\n",
    "                    FP_run.append(FP)\n",
    "                    total_spindle_run.append(total_spindle_count)\n",
    "                    prev_f1 = sum(f1_mean_run)/len(f1_mean_run)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.float() / len(dataloaders[phase].dataset)\n",
    "            \n",
    "            prefix = ''\n",
    "            if phase == 'validation':\n",
    "                prefix = 'val_'\n",
    "\n",
    "            logs[prefix + 'log loss'] = epoch_loss.item()\n",
    "            logs[prefix + 'accuracy'] = epoch_acc.item()\n",
    "        \n",
    "        liveloss.update(logs)\n",
    "        liveloss.send()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecedaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = u_net_backbone()\n",
    "criterion = GeneralizedDiceLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.005)\n",
    "train_model(model, criterion, optimizer, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "52b6caf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:0\n",
      "TRAINING\n",
      "Loss:  0.744606\n",
      "F1 MEAN: 0.290685  F1 STD: 0.256486  TP: 1912  FP: 618  Number of spindles: 4172\n",
      "VALIDATION\n",
      "Loss:  0.741345\n",
      "0.7907912731170654\n",
      "F1 MEAN: 0.261787  F1 STD: 0.253552  TP: 347  FP: 130  Number of spindles: 842\n",
      "\n",
      "EPOCH:1\n",
      "TRAINING\n",
      "Loss:  0.692779\n",
      "VALIDATION\n",
      "Loss:  0.729722\n",
      "\n",
      "EPOCH:2\n",
      "TRAINING\n",
      "Loss:  0.691995\n",
      "VALIDATION\n",
      "Loss:  0.726978\n",
      "\n",
      "EPOCH:3\n",
      "TRAINING\n",
      "Loss:  0.69353\n",
      "VALIDATION\n",
      "Loss:  0.696772\n",
      "\n",
      "EPOCH:4\n",
      "TRAINING\n",
      "Loss:  0.688948\n",
      "VALIDATION\n",
      "Loss:  0.748856\n",
      "\n",
      "EPOCH:5\n",
      "TRAINING\n",
      "Loss:  0.685562\n",
      "VALIDATION\n",
      "Loss:  0.737462\n",
      "\n",
      "EPOCH:6\n",
      "TRAINING\n",
      "Loss:  0.683325\n",
      "VALIDATION\n",
      "Loss:  0.732291\n",
      "\n",
      "EPOCH:7\n",
      "TRAINING\n",
      "Loss:  0.680534\n",
      "VALIDATION\n",
      "Loss:  0.692674\n",
      "\n",
      "EPOCH:8\n",
      "TRAINING\n",
      "Loss:  0.679072\n",
      "VALIDATION\n",
      "Loss:  0.702669\n",
      "\n",
      "EPOCH:9\n",
      "TRAINING\n",
      "Loss:  0.674556\n",
      "VALIDATION\n",
      "Loss:  0.72547\n",
      "\n",
      "EPOCH:10\n",
      "TRAINING\n",
      "Loss:  0.674944\n",
      "VALIDATION\n",
      "Loss:  0.721605\n",
      "\n",
      "EPOCH:11\n",
      "TRAINING\n",
      "Loss:  0.673518\n",
      "VALIDATION\n",
      "Loss:  0.737867\n",
      "\n",
      "EPOCH:12\n",
      "TRAINING\n",
      "Loss:  0.674038\n",
      "VALIDATION\n",
      "Loss:  0.726156\n",
      "\n",
      "EPOCH:13\n",
      "TRAINING\n",
      "Loss:  0.671856\n",
      "VALIDATION\n",
      "Loss:  0.728635\n",
      "\n",
      "EPOCH:14\n",
      "TRAINING\n",
      "Loss:  0.670822\n",
      "VALIDATION\n",
      "Loss:  0.684087\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3517911/3586803809.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;31m#torch.save(net, '/home/marius/Documents/OneDrive/MSc/StartUP/Code/m1_stats_features.pt')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Finished Training'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3517911/3586803809.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(BATCH_SIZE, EPOCHS)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mFP_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mtotal_spindle_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/s174411/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/s174411/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/s174411/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1280\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1282\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1283\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/s174411/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/s174411/lib/python3.7/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/s174411/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/s174411/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/s174411/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/s174411/lib/python3.7/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Loading data, setting up GPU use, setting up variables for model training\n",
    "def main(BATCH_SIZE = 12, EPOCHS = 801):\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    \n",
    "\n",
    "    training_loss = []\n",
    "    validation_loss = []\n",
    "    for j, epoch in enumerate(range(EPOCHS)):  # loop over the dataset multiple times\n",
    "        net.train()\n",
    "\n",
    "        running_loss = []\n",
    "        \n",
    "        for i, batch in enumerate(data_loader_train):\n",
    "            \n",
    "                \n",
    "            model_input, labels = batch\n",
    "            model_input = model_input.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(model_input)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss.append(loss.item())\n",
    "\n",
    "            \n",
    "\n",
    "        print(f\"EPOCH:{epoch}\")\n",
    "        print(\"TRAINING\")\n",
    "        print(\"Loss: \", round(sum(running_loss)/len(running_loss), 6))\n",
    "        training_loss.append(sum(running_loss)/len(running_loss))\n",
    "        \n",
    "        if (epoch % 100 == 0):\n",
    "            print(\"F1 MEAN:\", round(sum(f1_mean_run)/len(f1_mean_run), 6), \" F1 STD:\", round(sum(f1_std_run)/len(f1_std_run), 6), \" TP:\", sum(TP_run), \" FP:\", sum(FP_run),\n",
    "                \" Number of spindles:\", sum(total_spindle_run))\n",
    "\n",
    "        net.eval()\n",
    "        \n",
    "        running_loss = []\n",
    "        print(\"VALIDATION\")\n",
    "        running_loss = []\n",
    "\n",
    "        for i, batch in enumerate(data_loader_val): \n",
    "            model_input, labels = batch\n",
    "            model_input = model_input.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = net(model_input)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss.append(loss.item())\n",
    "\n",
    "            if (epoch % 100 == 0):\n",
    "                f1_mean, f1_std, TP, FP, total_spindle_count = f1_score(outputs, labels)\n",
    "                f1_mean_run.append(f1_mean)\n",
    "                f1_std_run.append(f1_std)\n",
    "                TP_run.append(TP)\n",
    "                FP_run.append(FP)\n",
    "                total_spindle_run.append(total_spindle_count)\n",
    "\n",
    "\n",
    "        print(\"Loss: \", round(sum(running_loss)/len(running_loss), 6))\n",
    "        if (epoch % 100 == 0):\n",
    "            print(loss.item())\n",
    "            print(\"F1 MEAN:\", round(sum(f1_mean_run)/len(f1_mean_run), 6), \" F1 STD:\", round(sum(f1_std_run)/len(f1_std_run), 6), \" TP:\", sum(TP_run), \" FP:\", sum(FP_run),\n",
    "                \" Number of spindles:\", sum(total_spindle_run))\n",
    "        print(\"\")\n",
    "        \n",
    "        validation_loss.append(sum(running_loss)/len(running_loss))\n",
    "\n",
    "\n",
    "\n",
    "main()\n",
    "#torch.save(net, '/home/marius/Documents/OneDrive/MSc/StartUP/Code/m1_stats_features.pt')\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e16619",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
